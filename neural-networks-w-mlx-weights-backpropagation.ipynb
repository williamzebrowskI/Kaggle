{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Neural Network Fundamentals with MLX: From Weights to Backpropagation \n","\n","Welcome to this step-by-step tutorial on building and evaluating a neural network using the MLX framework. In this notebook, we will guide you through the fundamentals of neural networks, covering everything from weight initialization and forward propagation to loss calculation and optimization. By the end of this tutorial, you will have a solid understanding of how to implement and evaluate a simple neural network for a regression task. Letâ€™s dive in and explore the fascinating world of machine learning with MLX!\n"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-14T20:53:26.978857Z","iopub.status.busy":"2024-07-14T20:53:26.978492Z","iopub.status.idle":"2024-07-14T20:53:45.125801Z","shell.execute_reply":"2024-07-14T20:53:45.124170Z","shell.execute_reply.started":"2024-07-14T20:53:26.978826Z"},"trusted":true},"outputs":[],"source":["!pip install -q mlx numpy"]},{"cell_type":"markdown","metadata":{},"source":["## Proper Imports"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.129053Z","iopub.status.busy":"2024-07-14T20:53:45.128645Z","iopub.status.idle":"2024-07-14T20:53:45.159074Z","shell.execute_reply":"2024-07-14T20:53:45.157925Z","shell.execute_reply.started":"2024-07-14T20:53:45.129011Z"},"trusted":true},"outputs":[],"source":["import mlx.core as mx\n","import mlx.nn as nn\n","import numpy as np\n","\n","import mlx.optimizers as optim"]},{"cell_type":"markdown","metadata":{},"source":["# Define the NN Model\n","\n","## Don't worry, we'll break it down below"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.165001Z","iopub.status.busy":"2024-07-14T20:53:45.164647Z","iopub.status.idle":"2024-07-14T20:53:45.181842Z","shell.execute_reply":"2024-07-14T20:53:45.180476Z","shell.execute_reply.started":"2024-07-14T20:53:45.164970Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["layer sizes: [3, 2, 1]\n","Layer sizes except the last: [3, 2]\n","Layer sizes except the first: [2, 1]\n","(2, 3)\n","\n","\n","Layer 0 weights shape: (2, 3)\n","Layer 0 weights:\n","array([[0.184734, 0.429828, 0.356261],\n","       [-0.0729912, -0.254855, -0.107632]], dtype=float32)\n","Layer 0 biases shape: (2,)\n","Layer 0 biases: array([-0.413167, -0.145139], dtype=float32)\n","\n","\n","Layer 1 weights shape: (1, 2)\n","Layer 1 weights:\n","array([[0.119513, 0.522265]], dtype=float32)\n","Layer 1 biases shape: (1,)\n","Layer 1 biases: array([0.583146], dtype=float32)\n"]}],"source":["# Define the MLP Model\n","class MLP(nn.Module):\n","    def __init__(self, num_layers: int, input_dim: int, hidden_dim: int, output_dim: int):\n","        super().__init__()\n","        layer_sizes = [input_dim] + [hidden_dim] * num_layers + [output_dim]\n","        print(\"layer sizes:\", layer_sizes)\n","        \n","        print(\"Layer sizes except the last:\", layer_sizes[:-1])\n","        print(\"Layer sizes except the first:\", layer_sizes[1:])\n","        \n","        self.layers = [\n","            nn.Linear(idim, odim)\n","            for idim, odim in zip(layer_sizes[:-1], layer_sizes[1:])\n","        ]\n","        print(self.layers[0].weight.shape)\n","        \n","        for i, layer in enumerate(self.layers):\n","            print(\"\\n\")\n","            print(f\"Layer {i} weights shape: {layer.weight.shape}\")\n","            print(f\"Layer {i} weights:\\n{layer.weight}\")\n","            print(f\"Layer {i} biases shape: {layer.bias.shape}\")\n","            print(f\"Layer {i} biases: {layer.bias}\")\n","\n","    def __call__(self, x):\n","        for i, l in enumerate(self.layers[:-1]):\n","            z = l(x)\n","            print(f\"\\nLayer {i} linear output (before ReLU):\\n{z}\")\n","            x = mx.maximum(z, 0.0)\n","            print(f\"Layer {i} output (after ReLU):\\n{x}\")\n","        z = self.layers[-1](x)\n","        print(f\"\\nOutput layer linear output:\\n{z}\")\n","        return z\n","    \n","# Example usage\n","input_dim = 3\n","hidden_dim = 2\n","output_dim = 1 \n","num_layers = 1\n","\n","# Initialize the model\n","model = MLP(num_layers, input_dim, hidden_dim, output_dim)"]},{"cell_type":"markdown","metadata":{},"source":["# Weights"]},{"cell_type":"markdown","metadata":{},"source":["The `Weights` matrix shape will always be (hidden_dim, input_dim). This is a 2D matrix where input_dim represents the number of input features, and hidden_dim represents the number of neurons in the hidden layer.\n","\n","Weight Matrix shape - i.e. (hidden_dim, input_dim)\n","   * `hidden_dim` is the number of columns (or neurons in the hidden layer)\n","   * `input_dim` is the number of rows (or features). Each column of the weight matrix corresponds to one input feature."]},{"cell_type":"markdown","metadata":{},"source":["# Bias\n","\n","The `Bias` vector shape will always be (1, hidden_dim). This is a 1D array where the number of elements is equal to hidden_dim."]},{"cell_type":"markdown","metadata":{},"source":["## Let's break this down to show exactly what happens when a input is put through the network.\n","\n","We have a input X (array) of 3 features:"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.185776Z","iopub.status.busy":"2024-07-14T20:53:45.185361Z","iopub.status.idle":"2024-07-14T20:53:45.192767Z","shell.execute_reply":"2024-07-14T20:53:45.191541Z","shell.execute_reply.started":"2024-07-14T20:53:45.185745Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Input X:\n"," array([[0.5, -1.2, 3.3]], dtype=float32)\n"]}],"source":["X = mx.array(np.array([[0.5, -1.2, 3.3]]).astype(np.float32))\n","print(\"Input X:\\n\", X)"]},{"cell_type":"markdown","metadata":{},"source":["The `weight` matrix and `bias` for the first layer was created when we intialized our model:"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.194599Z","iopub.status.busy":"2024-07-14T20:53:45.194220Z","iopub.status.idle":"2024-07-14T20:53:45.205545Z","shell.execute_reply":"2024-07-14T20:53:45.204224Z","shell.execute_reply.started":"2024-07-14T20:53:45.194569Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["weight shape: (2, 3)\n","weight:\n"," array([[0.184734, 0.429828, 0.356261],\n","       [-0.0729912, -0.254855, -0.107632]], dtype=float32)\n","bias:\n"," array([-0.413167, -0.145139], dtype=float32)\n"]}],"source":["print(\"weight shape:\", model.layers[0].weight.shape)\n","\n","W_0 = model.layers[0].weight\n","print(\"weight:\\n\", W_0)\n","\n","B_0 = model.layers[0].bias\n","print(\"bias:\\n\", B_0)"]},{"cell_type":"markdown","metadata":{},"source":["Matrix Multiplication is used in NN's to compute the weighted sum of inputs for each neuron in a layer.\n","\n","* Input X is of one dimension (1, input_dim) - an array.\n","\n","* The weight matrix is transposed, `mx.transpose(W_0)`, to properly align with the matrix with the one dimentional input (X).\n","    * The rows now become columns so, it went from (2, 3) to (3, 2)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.207698Z","iopub.status.busy":"2024-07-14T20:53:45.207293Z","iopub.status.idle":"2024-07-14T20:53:45.216904Z","shell.execute_reply":"2024-07-14T20:53:45.215680Z","shell.execute_reply.started":"2024-07-14T20:53:45.207666Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["W Transposed:\n","  array([[0.184734, -0.0729912],\n","       [0.429828, -0.254855],\n","       [0.356261, -0.107632]], dtype=float32)\n"]}],"source":["W_0T = mx.transpose(W_0)\n","print(\"W Transposed:\\n \", W_0T)"]},{"cell_type":"markdown","metadata":{},"source":["Now, we'll do the matrix multiplication with `X` and `W_0T`!"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.218884Z","iopub.status.busy":"2024-07-14T20:53:45.218484Z","iopub.status.idle":"2024-07-14T20:53:45.230224Z","shell.execute_reply":"2024-07-14T20:53:45.227914Z","shell.execute_reply.started":"2024-07-14T20:53:45.218853Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["input:\n"," array([[0.5, -1.2, 3.3]], dtype=float32)\n","\n","W transposed:\n"," array([[0.184734, -0.0729912],\n","       [0.429828, -0.254855],\n","       [0.356261, -0.107632]], dtype=float32)\n","\n","ouput:\n"," array([[0.752234, -0.0858543]], dtype=float32)\n"]}],"source":["print(\"input:\\n\", X)\n","\n","print(\"\\nW transposed:\\n\", W_0T)\n","\n","Z_0 = mx.matmul(X, W_0T)\n","\n","print(\"\\nouput:\\n\", Z_0)"]},{"cell_type":"code","execution_count":10,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2024-07-14T20:53:45.232960Z","iopub.status.busy":"2024-07-14T20:53:45.232579Z","iopub.status.idle":"2024-07-14T20:53:45.294335Z","shell.execute_reply":"2024-07-14T20:53:45.293151Z","shell.execute_reply.started":"2024-07-14T20:53:45.232929Z"},"trusted":true},"outputs":[{"data":{"text/markdown":["What you see there is each element in the Input array being multiplied with an element in a column."],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["### 1A. Multiply with the first column:"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["* **0.5 * 0.185 = 0.092**"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["* **-1.200 * 0.430 = -0.516**"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["* **3.300 * 0.356 = 1.176**"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["### 2A. Sum these products:\n"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["* **0.092 + -0.516 + 1.176 = 0.752**"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["\n","### 1B. Multiply with the second column:"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["* **0.5 * -0.073 = -0.036**"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["* **-1.200 * -0.255 = 0.306**"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["* **3.300 * -0.108 = -0.355**"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["### 2B. Sum these products:\n"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["* **-0.036 + 0.306 + -0.355 = -0.086** "],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["\n","\n"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["### As you can see, we did the dot product manually we can make sure our sum_results match the actual mx.matmul dot product:  "],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["**sum_result_0 = 0.752**"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["**sum_result_1 = -0.086**"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["**mx.matmul result from above, Z_0 = array([[0.752234, -0.0858543]], dtype=float32)**"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["from IPython.display import display, Markdown\n","\n","# Extract and format the values for the first column\n","x0_w0_0 = X[0][0].item() * W_0T[0][0].item()\n","x1_w1_0 = X[0][1].item() * W_0T[1][0].item()\n","x2_w2_0 = X[0][2].item() * W_0T[2][0].item()\n","sum_result_0 = x0_w0_0 + x1_w1_0 + x2_w2_0\n","\n","# Extract and format the values for the second column\n","x0_w0_1 = X[0][0].item() * W_0T[0][1].item()\n","x1_w1_1 = X[0][1].item() * W_0T[1][1].item()\n","x2_w2_1 = X[0][2].item() * W_0T[2][1].item()\n","sum_result_1 = x0_w0_1 + x1_w1_1 + x2_w2_1\n","\n","display(Markdown(\"What you see there is each element in the Input array being multiplied with an element in a column.\"))\n","\n","display(Markdown(\"### 1A. Multiply with the first column:\"))\n","display(Markdown(f\"* **{X[0][0].item()} * {W_0T[0][0].item():.3f} = {x0_w0_0:.3f}**\"))\n","display(Markdown(f\"* **{X[0][1].item():.3f} * {W_0T[1][0].item():.3f} = {x1_w1_0:.3f}**\"))\n","display(Markdown(f\"* **{X[0][2].item():.3f} * {W_0T[2][0].item():.3f} = {x2_w2_0:.3f}**\"))\n","\n","display(Markdown(\"### 2A. Sum these products:\\n\"))\n","display(Markdown(f\"* **{x0_w0_0:.3f} + {x1_w1_0:.3f} + {x2_w2_0:.3f} = {sum_result_0:.3f}**\"))\n","\n","display(Markdown(\"\\n### 1B. Multiply with the second column:\"))\n","display(Markdown(f\"* **{X[0][0].item()} * {W_0T[0][1].item():.3f} = {x0_w0_1:.3f}**\"))\n","display(Markdown(f\"* **{X[0][1].item():.3f} * {W_0T[1][1].item():.3f} = {x1_w1_1:.3f}**\"))\n","display(Markdown(f\"* **{X[0][2].item():.3f} * {W_0T[2][1].item():.3f} = {x2_w2_1:.3f}**\"))\n","\n","display(Markdown(\"### 2B. Sum these products:\\n\"))\n","display(Markdown(f\"* **{x0_w0_1:.3f} + {x1_w1_1:.3f} + {x2_w2_1:.3f} = {sum_result_1:.3f}** \"))\n","\n","display(Markdown(\"\\n\\n\"))\n","display(Markdown(\"### As you can see, we did the dot product manually we can make sure our sum_results match the actual mx.matmul dot product:  \"))\n","\n","\n","display(Markdown(f\"**sum_result_0 = {sum_result_0:.3f}**\"))\n","display(Markdown(f\"**sum_result_1 = {sum_result_1:.3f}**\"))\n","display(Markdown(f\"**mx.matmul result from above, Z_0 = {Z_0}**\"))\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Finally, we add the `Bias` to Z_0"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.296135Z","iopub.status.busy":"2024-07-14T20:53:45.295772Z","iopub.status.idle":"2024-07-14T20:53:45.303692Z","shell.execute_reply":"2024-07-14T20:53:45.302173Z","shell.execute_reply.started":"2024-07-14T20:53:45.296103Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Bias:\n"," array([-0.413167, -0.145139], dtype=float32)\n","\n","Dot products:\n"," array([[0.752234, -0.0858543]], dtype=float32)\n","\n","Linear Output:\n"," array([[0.339067, -0.230994]], dtype=float32)\n"]}],"source":["print(\"Bias:\\n\", B_0)\n","\n","print(\"\\nDot products:\\n\", Z_0)\n","\n","\n","Z_0 = Z_0 + B_0\n","print(\"\\nLinear Output:\\n\", Z_0)"]},{"cell_type":"markdown","metadata":{},"source":["You'll see above, that we added the first element of the bias to the first element of the dot products and the same for the 2nd element of each to get the Lienar output.\n","\n","After the matrix multiplication (dot product) and addition of the bias, the resulting output is passed through an activation function. "]},{"cell_type":"markdown","metadata":{},"source":["# ReLU activation function\n","\n","The purpose of the `activation function` is to introduce non-linearity into the model, allowing it to learn more complex patterns.\n","\n","This operation compares each element of  Z_0  with 0 and returns the maximum of the two values."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.305747Z","iopub.status.busy":"2024-07-14T20:53:45.305334Z","iopub.status.idle":"2024-07-14T20:53:45.325250Z","shell.execute_reply":"2024-07-14T20:53:45.323582Z","shell.execute_reply.started":"2024-07-14T20:53:45.305701Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([[0.339067, 0]], dtype=float32)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["relu_output = mx.maximum(Z_0, 0.0)\n","\n","relu_output"]},{"cell_type":"markdown","metadata":{},"source":["Now, that we have the output of the ReLU fuction above.  We pass this through our next layer.  Let's grab the next layers weights and biases."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.326967Z","iopub.status.busy":"2024-07-14T20:53:45.326609Z","iopub.status.idle":"2024-07-14T20:53:45.336582Z","shell.execute_reply":"2024-07-14T20:53:45.335116Z","shell.execute_reply.started":"2024-07-14T20:53:45.326938Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["weight shape: (1, 2)\n","weight:\n"," array([[0.119513, 0.522265]], dtype=float32)\n","bias:\n"," array([0.583146], dtype=float32)\n"]}],"source":["print(\"weight shape:\", model.layers[1].weight.shape)\n","\n","W_1 = model.layers[1].weight\n","print(\"weight:\\n\", W_1)\n","\n","B_1 = model.layers[1].bias\n","print(\"bias:\\n\", B_1)"]},{"cell_type":"markdown","metadata":{},"source":["Let's grab the relu_output from above and pass it apply matrix multiplication.\n","\n","Note:\n","\n","1. First, we'll Transpose W_1 to align with matrix muliplication with the `relu_output` \n","2. Since this is a single hidden layer network, this will be going towards the output for prediction."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.338585Z","iopub.status.busy":"2024-07-14T20:53:45.338225Z","iopub.status.idle":"2024-07-14T20:53:45.358207Z","shell.execute_reply":"2024-07-14T20:53:45.356965Z","shell.execute_reply.started":"2024-07-14T20:53:45.338554Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["W_1 Transposed:\n"," array([[0.119513],\n","       [0.522265]], dtype=float32)\n","relu_output:\n"," array([[0.339067, 0]], dtype=float32)\n"]}],"source":["# Transpose W_1.\n","W_1T = mx.transpose(W_1)\n","print(\"W_1 Transposed:\\n\", W_1T)\n","\n","# Relu output from first hidden layer:\n","print(\"relu_output:\\n\", relu_output)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Apply MatMul:"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.360269Z","iopub.status.busy":"2024-07-14T20:53:45.359812Z","iopub.status.idle":"2024-07-14T20:53:45.370738Z","shell.execute_reply":"2024-07-14T20:53:45.369495Z","shell.execute_reply.started":"2024-07-14T20:53:45.360225Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dot product (Z_1) before adding bias:\n","\n"," array([[0.0405229]], dtype=float32)\n"]}],"source":["Z_1 = mx.matmul(relu_output, W_1T)\n","print(\"Dot product (Z_1) before adding bias:\\n\\n\", Z_1)"]},{"cell_type":"markdown","metadata":{},"source":["## Adding Bias:"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.379255Z","iopub.status.busy":"2024-07-14T20:53:45.378873Z","iopub.status.idle":"2024-07-14T20:53:45.386484Z","shell.execute_reply":"2024-07-14T20:53:45.385282Z","shell.execute_reply.started":"2024-07-14T20:53:45.379225Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Dot product:\n","\n"," array([[0.0405229]], dtype=float32)\n","\n","Bias:\n"," array([0.583146], dtype=float32)\n","\n","Output layer linear output (Z_1):\n"," array([[0.623668]], dtype=float32)\n"]}],"source":["print(\"\\nDot product:\\n\\n\", Z_1)\n","\n","print(\"\\nBias:\\n\", B_1)\n","\n","Z_1 += B_1\n","print(\"\\nOutput layer linear output (Z_1):\\n\", Z_1)"]},{"cell_type":"markdown","metadata":{},"source":["Now, to see if we did everything correctly.  We can match our hand written prediction under  `Output layer linear output` from above with the actual prediction when we call the `model`.\n","\n","Let's try it!"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.388432Z","iopub.status.busy":"2024-07-14T20:53:45.387876Z","iopub.status.idle":"2024-07-14T20:53:45.400488Z","shell.execute_reply":"2024-07-14T20:53:45.399275Z","shell.execute_reply.started":"2024-07-14T20:53:45.388383Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Layer 0 linear output (before ReLU):\n","array([[0.339067, -0.230994]], dtype=float32)\n","Layer 0 output (after ReLU):\n","array([[0.339067, 0]], dtype=float32)\n","\n","Output layer linear output:\n","array([[0.623668]], dtype=float32)\n","Model output: array([[0.623668]], dtype=float32)\n"]}],"source":["output = model(X)\n","print(\"Model output:\", output)"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, our model output array maches our prediction, `array([[-0.493743]]`\n","\n","**Ours:**\n","\n","    Dot product:\n","\n","     array([[-1.05947]], dtype=float32)\n","\n","    Bias:\n","     array([0.565723], dtype=float32)\n","\n","    Output layer linear output (Z_1):\n","     array([[-0.493743]], dtype=float32)\n","     \n","**Models prediction:**\n","\n","    Layer 0 linear output (before ReLU):\n","    array([[1.6477, -1.99371]], dtype=float32)\n","    Layer 0 output (after ReLU):\n","    array([[1.6477, 0]], dtype=float32)\n","\n","    Output layer linear output:\n","    array([[-0.493743]], dtype=float32)\n","    Model output: array([[-0.493743]], dtype=float32)"]},{"cell_type":"markdown","metadata":{},"source":["A neat way to check the models parameters (weights and biases for each layer) is `model.parameters()`"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.402450Z","iopub.status.busy":"2024-07-14T20:53:45.402019Z","iopub.status.idle":"2024-07-14T20:53:45.415866Z","shell.execute_reply":"2024-07-14T20:53:45.414718Z","shell.execute_reply.started":"2024-07-14T20:53:45.402385Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'layers': [{'weight': array([[0.184734, 0.429828, 0.356261],\n","          [-0.0729912, -0.254855, -0.107632]], dtype=float32),\n","   'bias': array([-0.413167, -0.145139], dtype=float32)},\n","  {'weight': array([[0.119513, 0.522265]], dtype=float32),\n","   'bias': array([0.583146], dtype=float32)}]}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["mx.eval(model.parameters())\n","model.parameters()"]},{"cell_type":"markdown","metadata":{},"source":["## Loss functions\n","\n","Now that we have our prediction, let's introduce the Loss function into the mix here. We can try a few different ones.\n","\n","Loss functions:\n","\n","1. Mean Squared Error Loss\n","\n","2. Cross Entropy Loss"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.418101Z","iopub.status.busy":"2024-07-14T20:53:45.417626Z","iopub.status.idle":"2024-07-14T20:53:45.427748Z","shell.execute_reply":"2024-07-14T20:53:45.426136Z","shell.execute_reply.started":"2024-07-14T20:53:45.418059Z"},"trusted":true},"outputs":[],"source":["def mse_loss(y_pred, y_true):\n","    return mx.mean((y_pred - y_true) ** 2)\n","\n","def loss_fn(model, X, y):\n","    return mx.mean(nn.losses.cross_entropy(model(X), y))"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.431113Z","iopub.status.busy":"2024-07-14T20:53:45.429608Z","iopub.status.idle":"2024-07-14T20:53:45.442327Z","shell.execute_reply":"2024-07-14T20:53:45.441000Z","shell.execute_reply.started":"2024-07-14T20:53:45.431073Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["dummy ground truth:\n"," array([[0.2345]], dtype=float32)\n","\n","predicted value from above:\n"," array([[0.623668]], dtype=float32)\n"]}],"source":["# Dummy ground truth value\n","y_true = mx.array(np.array([[0.2345]]).astype(np.float32))\n","print(\"dummy ground truth:\\n\", y_true)\n","\n","print(\"\\npredicted value from above:\\n\", Z_1 )\n"]},{"cell_type":"markdown","metadata":{},"source":["Calculate the loss - how far is our prediction from the ground truth?\n","\n","1. Mean Error Squared:\n","\n","    * Measures how close the predicted values are to the ground truth."]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.444222Z","iopub.status.busy":"2024-07-14T20:53:45.443762Z","iopub.status.idle":"2024-07-14T20:53:45.454508Z","shell.execute_reply":"2024-07-14T20:53:45.453331Z","shell.execute_reply.started":"2024-07-14T20:53:45.444178Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss: 0.151\n"]}],"source":["loss = mse_loss(Z_1, y_true)\n","print(f\"Loss: {loss.item():.3f}\")"]},{"cell_type":"markdown","metadata":{},"source":["2. Cross Entropy Loss:\n","\n","When to Use: Use CE for classification problems where the goal is to predict the probability distribution over a set of classes. Cross Ent is typically used in the `Transformers` arcitecture, like in `BERT` and `GPT` for sequence-to-sequence tasks.\n","\n","   *  Measures how well the predicted class probabilities match the true class labels."]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.456386Z","iopub.status.busy":"2024-07-14T20:53:45.456001Z","iopub.status.idle":"2024-07-14T20:53:45.467587Z","shell.execute_reply":"2024-07-14T20:53:45.466290Z","shell.execute_reply.started":"2024-07-14T20:53:45.456350Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Layer 0 linear output (before ReLU):\n","array([[0.339067, -0.230994]], dtype=float32)\n","Layer 0 output (after ReLU):\n","array([[0.339067, 0]], dtype=float32)\n","\n","Output layer linear output:\n","array([[0.623668]], dtype=float32)\n","\n","loss: 0.477\n"]}],"source":["ce_loss = loss_fn(model, X, y_true)\n","print(f\"\\nloss: {ce_loss.item():.3f}\")"]},{"cell_type":"markdown","metadata":{},"source":["### After the loss is calculated, the next step is to perform backpropagation and optimization. \n","\n","#### Backpropagation is the process of calculating the gradient of the loss function with respect to each weight and bias in the network. These gradients indicate how much the loss would change with a small change in each parameter. \n","\n","Note: we'll be using the cross entropy loss function going forward."]},{"cell_type":"markdown","metadata":{},"source":["The `mlx.nn.value_and_grad()` function is a convenience function to get the gradient of a loss with respect to the trainable parameters of a model. This should not be confused with `mlx.core.value_and_grad()`."]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.469961Z","iopub.status.busy":"2024-07-14T20:53:45.469486Z","iopub.status.idle":"2024-07-14T20:53:45.482935Z","shell.execute_reply":"2024-07-14T20:53:45.481255Z","shell.execute_reply.started":"2024-07-14T20:53:45.469918Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model:\n"," MLP(\n","  (layers.0): Linear(input_dims=3, output_dims=2, bias=True)\n","  (layers.1): Linear(input_dims=2, output_dims=1, bias=True)\n",")\n","Loss: 0.477\n","\n","\n","Per our print statments in our forward method: \n","\n","Layer 0 linear output (before ReLU):\n","array([[0.339067, -0.230994]], dtype=float32)\n","Layer 0 output (after ReLU):\n","array([[0.339067, 0]], dtype=float32)\n","\n","Output layer linear output:\n","array([[0.623668]], dtype=float32)\n","\n","\n","Grads: {'layers': [{'weight': array([[0.0457436, -0.109785, 0.301908],\n","       [0, 0, 0]], dtype=float32), 'bias': array([0.0914872, 0], dtype=float32)}, {'weight': array([[0.259556, 0]], dtype=float32), 'bias': array([0.7655], dtype=float32)}]}\n","\n","Let's break down the Grads (gradiants)\n"]}],"source":["# Let's print out what the model and loss.\n","print(\"model:\\n\", model)\n","print(f\"Loss: {ce_loss.item():.3f}\")\n","\n","# Compute the loss and gradients\n","print(\"\\n\\nPer our print statments in our forward method: \")\n","loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n","loss, grads = loss_and_grad_fn(model, X, y_true)\n","\n","print(f\"\\n\\nGrads: {grads}\")\n","\n","print(\"\\nLet's break down the Grads (gradiants)\")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.485539Z","iopub.status.busy":"2024-07-14T20:53:45.484506Z","iopub.status.idle":"2024-07-14T20:53:45.494766Z","shell.execute_reply":"2024-07-14T20:53:45.493528Z","shell.execute_reply.started":"2024-07-14T20:53:45.485499Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'weight': array([[0.0457436, -0.109785, 0.301908],\n","        [0, 0, 0]], dtype=float32),\n"," 'bias': array([0.0914872, 0], dtype=float32)}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["first_layer_grads = grads['layers'][0]\n","first_layer_grads"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.496481Z","iopub.status.busy":"2024-07-14T20:53:45.496088Z","iopub.status.idle":"2024-07-14T20:53:45.508369Z","shell.execute_reply":"2024-07-14T20:53:45.506773Z","shell.execute_reply.started":"2024-07-14T20:53:45.496435Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Gradients of Layer 0 weights:\n","array([[0.0457436, -0.109785, 0.301908],\n","       [0, 0, 0]], dtype=float32)\n","\n","\n","Gradients of Layer 0 biases:\n","array([0.0914872, 0], dtype=float32)\n","Gradients of Layer 1 weights:\n","array([[0.259556, 0]], dtype=float32)\n","\n","\n","Gradients of Layer 1 biases:\n","array([0.7655], dtype=float32)\n"]}],"source":["# Print gradients for each layer\n","for i, layer in enumerate(model.layers):\n","    print(f\"Gradients of Layer {i} weights:\\n{grads['layers'][i]['weight']}\")\n","    print(f\"\\n\\nGradients of Layer {i} biases:\\n{grads['layers'][i]['bias']}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Instantiate the optimizer\n","\n","Now, that we have our gradiants, we are going to define our optimizer, which is reponsibly for updating the model's params (weights and biases) to miniize the loss function next iteration.\n","\n","## We'll use  Stochastic Gradient Descent (SGD) and define a learning rate that the optimizer will use learning rate to control the step size of the parameter updates."]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.510563Z","iopub.status.busy":"2024-07-14T20:53:45.510152Z","iopub.status.idle":"2024-07-14T20:53:45.521271Z","shell.execute_reply":"2024-07-14T20:53:45.519971Z","shell.execute_reply.started":"2024-07-14T20:53:45.510530Z"},"trusted":true},"outputs":[],"source":["learning_rate = 0.1\n","\n","optimizer = optim.SGD(learning_rate=learning_rate)"]},{"cell_type":"markdown","metadata":{},"source":["Let's take a look at the original weights and biases that we talkd about earlier:"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.524051Z","iopub.status.busy":"2024-07-14T20:53:45.522779Z","iopub.status.idle":"2024-07-14T20:53:45.539953Z","shell.execute_reply":"2024-07-14T20:53:45.538476Z","shell.execute_reply.started":"2024-07-14T20:53:45.523999Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["layer 0 weight:\n","array([[0.184734, 0.429828, 0.356261],\n","       [-0.0729912, -0.254855, -0.107632]], dtype=float32)\n","\n","layer 0 bias:\n","array([-0.413167, -0.145139], dtype=float32)\n","\n","layer 1 weights:\n","array([[0.119513, 0.522265]], dtype=float32)\n","\n","layer 1 bias:\n","array([0.583146], dtype=float32)\n"]}],"source":["print(f\"layer 0 weight:\\n{W_0}\")\n","\n","print(f\"\\nlayer 0 bias:\\n{B_0}\")\n","\n","print(f\"\\nlayer 1 weights:\\n{W_1}\")\n","      \n","print(f\"\\nlayer 1 bias:\\n{B_1}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Now, we update these with the `optimizer.update()` method and print out the updated weights and baises for the layers:"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.542092Z","iopub.status.busy":"2024-07-14T20:53:45.541611Z","iopub.status.idle":"2024-07-14T20:53:45.554726Z","shell.execute_reply":"2024-07-14T20:53:45.553461Z","shell.execute_reply.started":"2024-07-14T20:53:45.542033Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Updated Layer 0 weights:\n","array([[0.18016, 0.440807, 0.32607],\n","       [-0.0729912, -0.254855, -0.107632]], dtype=float32)\n","Updated Layer 0 biases:\n","array([-0.422316, -0.145139], dtype=float32)\n","\n","Updated Layer 1 weights:\n","array([[0.0935574, 0.522265]], dtype=float32)\n","Updated Layer 1 biases:\n","array([0.506596], dtype=float32)\n"]}],"source":["optimizer.update(model, grads)\n","\n","for i, layer in enumerate(model.layers):\n","    print(f\"\\nUpdated Layer {i} weights:\\n{layer.weight}\")\n","    print(f\"Updated Layer {i} biases:\\n{layer.bias}\")"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-07-13T21:07:44.078635Z","iopub.status.busy":"2024-07-13T21:07:44.077770Z","iopub.status.idle":"2024-07-13T21:07:44.085437Z","shell.execute_reply":"2024-07-13T21:07:44.083976Z","shell.execute_reply.started":"2024-07-13T21:07:44.078590Z"}},"source":["## We can now take a look at the models updated weights/biases - which should match what we just updated from above by using the `model.parameters()` method"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.556858Z","iopub.status.busy":"2024-07-14T20:53:45.556436Z","iopub.status.idle":"2024-07-14T20:53:45.568677Z","shell.execute_reply":"2024-07-14T20:53:45.567481Z","shell.execute_reply.started":"2024-07-14T20:53:45.556819Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'layers': [{'weight': array([[0.18016, 0.440807, 0.32607],\n","          [-0.0729912, -0.254855, -0.107632]], dtype=float32),\n","   'bias': array([-0.422316, -0.145139], dtype=float32)},\n","  {'weight': array([[0.0935574, 0.522265]], dtype=float32),\n","   'bias': array([0.506596], dtype=float32)}]}"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["model.parameters()"]},{"cell_type":"markdown","metadata":{},"source":["## Next, forcing evaluation provides immediate feedback on the results of the computations, which is useful for debugging and ensuring that the model is being updated correctly."]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.570442Z","iopub.status.busy":"2024-07-14T20:53:45.569996Z","iopub.status.idle":"2024-07-14T20:53:45.578883Z","shell.execute_reply":"2024-07-14T20:53:45.577688Z","shell.execute_reply.started":"2024-07-14T20:53:45.570376Z"},"trusted":true},"outputs":[],"source":["# Force a graph evaluation\n","mx.eval(model.parameters(), optimizer.state)"]},{"cell_type":"markdown","metadata":{},"source":["# We can actually calculate the accuracy of the models predictions to see how well it did!  \n","\n","Instead of using the Cross Entropy loss typically used for classification problems, we will define an evaluation function that uses Mean Squared Error (MSE) since we are dealing with a regression-like problem."]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.581591Z","iopub.status.busy":"2024-07-14T20:53:45.580581Z","iopub.status.idle":"2024-07-14T20:53:45.589711Z","shell.execute_reply":"2024-07-14T20:53:45.588484Z","shell.execute_reply.started":"2024-07-14T20:53:45.581546Z"},"trusted":true},"outputs":[],"source":["\n","def eval_fn(model, X, y):\n","    y_pred = model(X)\n","    # computes the Mean Squared Error between the predicted output y_pred and the ground truth y.\n","    return mse_loss(y_pred, y)"]},{"cell_type":"markdown","metadata":{},"source":["We will be putting in the input, which is `X` and evaluating with the ground truth `(y_true)`"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.591884Z","iopub.status.busy":"2024-07-14T20:53:45.591252Z","iopub.status.idle":"2024-07-14T20:53:45.602916Z","shell.execute_reply":"2024-07-14T20:53:45.601553Z","shell.execute_reply.started":"2024-07-14T20:53:45.591852Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Ground truth:\n","array([[0.2345]], dtype=float32)\n","\n","Layer 0 linear output (before ReLU):\n","array([[0.214827, -0.230994]], dtype=float32)\n","Layer 0 output (after ReLU):\n","array([[0.214827, 0]], dtype=float32)\n","\n","Output layer linear output:\n","array([[0.526694]], dtype=float32)\n","\n","accuracy: 0.085\n"]}],"source":["print(f\"\\n\\nGround truth:\\n{y_true}\")\n","\n","# Calculate accuracy\n","accuracy = eval_fn(model, X, y_true)\n","print(f\"\\naccuracy: {accuracy.item():.3f}\")"]},{"cell_type":"markdown","metadata":{},"source":["What we get is an accuracy (MSE) between:\n","\n","   * ground truth\n","   * Output layer linear output\n","   \n"," In this case. The lower the number, the more accurate our prediction is to the ground truth.  Obviously, the higher the further away."]},{"cell_type":"markdown","metadata":{},"source":["# Let's put it all together!\n","\n","## We'll use the individual compartments we used above and put them into on cell."]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T20:53:45.604795Z","iopub.status.busy":"2024-07-14T20:53:45.604444Z","iopub.status.idle":"2024-07-14T20:53:45.659753Z","shell.execute_reply":"2024-07-14T20:53:45.658521Z","shell.execute_reply.started":"2024-07-14T20:53:45.604756Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.173\n","Epoch 2, Loss: 0.167\n","Epoch 3, Loss: 0.162\n","Epoch 4, Loss: 0.157\n","Epoch 5, Loss: 0.151\n","Epoch 6, Loss: 0.147\n","Epoch 7, Loss: 0.142\n","Epoch 8, Loss: 0.137\n","Epoch 9, Loss: 0.133\n","Epoch 10, Loss: 0.128\n","Epoch 11, Loss: 0.124\n","Epoch 12, Loss: 0.120\n","Epoch 13, Loss: 0.116\n","Epoch 14, Loss: 0.112\n","Epoch 15, Loss: 0.108\n","Epoch 16, Loss: 0.105\n","Epoch 17, Loss: 0.101\n","Epoch 18, Loss: 0.098\n","Epoch 19, Loss: 0.095\n","Epoch 20, Loss: 0.091\n","Epoch 21, Loss: 0.088\n","Epoch 22, Loss: 0.085\n","Epoch 23, Loss: 0.082\n","Epoch 24, Loss: 0.080\n","Epoch 25, Loss: 0.077\n","Epoch 26, Loss: 0.074\n","Epoch 27, Loss: 0.072\n","Epoch 28, Loss: 0.069\n","Epoch 29, Loss: 0.067\n","Epoch 30, Loss: 0.065\n"]}],"source":["# The NN Model\n","class MLP(nn.Module):\n","    def __init__(self, num_layers: int, input_dim: int, hidden_dim: int, output_dim: int):\n","        super().__init__()\n","        layer_sizes = [input_dim] + [hidden_dim] * num_layers + [output_dim]\n","        self.layers = [\n","            nn.Linear(idim, odim)\n","            for idim, odim in zip(layer_sizes[:-1], layer_sizes[1:])\n","        ]\n","\n","    def __call__(self, x):\n","        for l in self.layers[:-1]:\n","            z = l(x)\n","            x = mx.maximum(z, 0.0)\n","        return self.layers[-1](x)\n","\n","# Example usage\n","input_dim = 3\n","hidden_dim = 2\n","output_dim = 1 \n","num_layers = 1\n","\n","# Initialize the model\n","model = MLP(num_layers, input_dim, hidden_dim, output_dim)\n","\n","# Loss MSE functions\n","def mse_loss(y_pred, y_true):\n","    return mx.mean((y_pred - y_true) ** 2)\n","\n","def loss_fn(model, X, y):\n","    y_pred = model(X)\n","    return mse_loss(y_pred, y)\n","\n","# Dummy ground truth value\n","y_true = mx.array(np.array([[0.2345]]).astype(np.float32))\n","X = mx.array(np.array([[0.5, -1.2, 3.3]]).astype(np.float32))\n","\n","# Instantiate the optimizer with a smaller learning rate\n","learning_rate = 0.001\n","optimizer = optim.SGD(learning_rate=learning_rate)\n","\n","for epoch in range(30):  # Run for a few epochs\n","    # Forward pass\n","    y_pred = model(X)\n","    \n","    # Calculate the loss\n","    loss = mse_loss(y_pred, y_true)\n","    print(f\"Epoch {epoch+1}, Loss: {loss.item():.3f}\")\n","    \n","    # Compute the gradients\n","    loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n","    _, grads = loss_and_grad_fn(model, X, y_true)\n","    \n","    # Update the model parameters\n","    optimizer.update(model, grads)"]},{"cell_type":"markdown","metadata":{},"source":["## We notice by the last epoch, our loss has gone down substantially from the first epoch."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
