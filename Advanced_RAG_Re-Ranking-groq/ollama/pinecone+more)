{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8884266,"sourceType":"datasetVersion","datasetId":5345830}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/william2020/advanced-rag-re-ranking-groq-ollama-pinecone-more?scriptVersionId=187583346\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Advanced RAG w/ Re-Ranking | Groq + Ollama + LangChain + Cohere + PineCone + Llama3-70B","metadata":{}},{"cell_type":"markdown","source":"This Jupyter notebook demonstrates an advanced implementation of the Retrieval-Augmented Generation (RAG) technique, enhanced with document re-ranking, to generate precise summaries based on a given query. The process involves several key steps:\n\n1. **Importing Libraries**: Sets up the environment by importing necessary Python libraries for PDF processing, text splitting, embeddings, and API interactions.\n2. **Language Model Setup with Groq**: Initializes a language model using Groq's platform, leveraging the Llama3 model with 8 billion parameters for generating responses.\n3. **PDF Text Processing**: Reads and processes text from a PDF document, splitting it into manageable chunks for further processing.\n4. **Document Embedding with Ollama**: Utilizes the Ollama embeddings to convert text chunks into vector representations.\n5. **Pinecone Indexing**: Sets up a Pinecone vector database for storing and querying document embeddings.\n6. **Query Embedding and Retrieval**: Embeds a user query for similarity search in the Pinecone index, retrieving relevant document chunks.\n7. **Re-Ranking with Cohere**: Applies Cohere's re-ranking model to refine the search results, ensuring the most relevant documents are selected.\n8. **Summary Generation**: Uses the Groq language model to generate a comprehensive summary based on the context provided by the re-ranked documents.\n\nThis notebook serves as a comprehensive guide for implementing a sophisticated RAG system with re-ranking, showcasing the integration of multiple AI and NLP technologies to enhance information retrieval and summarization tasks.","metadata":{}},{"cell_type":"code","source":"!pip install groq ollama cohere langchain pinecone-client PyPDF2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Before continuing, you'll need some API keys:","metadata":{}},{"cell_type":"code","source":"GROQ_API_KEY=\"\"\nPINECONE_API_KEY=\"\"\nCOHERE_API_KEY=\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import PyPDF2\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings import OllamaEmbeddings\nfrom langchain_groq import ChatGroq","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup Language Model with Groq\n\nLet's now set up the language model with Groq, Llama3 8 Billion Parameter","metadata":{}},{"cell_type":"code","source":"# llm_local = ChatOllama(model=\"mistral:instruct\")\nfrom os import getenv\nfrom groq import Groq\nfrom load_dotenv import load_dotenv\nload_dotenv()\n\nGROQ_API_KEY=getenv(\"GROQ_API_KEY\")\n\n\n\nllm_groq = ChatGroq(\n            groq_api_key=GROQ_API_KEY,\n            model_name='llama3:8b' \n            # model_name='mixtral-8x7b-32768'\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read and Split PDF Text\n\nReads text from a specified PDF file, concatenates it into a single string, and then splits the text into manageable chunks for processing.","metadata":{}},{"cell_type":"code","source":"# Read the PDF file\npdf = PyPDF2.PdfReader(\"/kaggle/input/merged-doc/Merged_Document.pdf\")\npdf_text = \"\"\nfor page in pdf.pages:\n    pdf_text += page.extract_text()\n\n# Split the text into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\ntexts = text_splitter.split_text(pdf_text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embed Documents with Ollama Embeddings\n\nLet's pull down a Ollama Embedding model.\n\nIf you have ollama install, run this in your terminal:\n\n`ollama pull nomic-embed-text`","metadata":{}},{"cell_type":"code","source":"embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n\nr1 = embeddings.embed_documents(\n    texts\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pinecone Index Setup and Upsert\n\nConfigures Pinecone for vector database operations, creates or connects to an index, and upserts document embeddings. Also defines a function to get query embeddings.","metadata":{}},{"cell_type":"code","source":"from os import getenv\nfrom load_dotenv import load_dotenv\nload_dotenv()\nfrom pinecone import Pinecone, ServerlessSpec\n\nPINECONE_API_KEY=getenv(\"PINECONE_API_KEY\")\n\npc = Pinecone(api_key=PINECONE_API_KEY)\n\nindex = pc.Index(\"ai-index\")\n\nfor i in range(len(texts)):\n    index.upsert([((str(i),r1[i],{\"text\":texts[i]}))])\n    \nprint(\"done upserting...\")\n\ndef get_query_embdedding(text):\n    embedding=embeddings.embed_query(text)\n    return embedding","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cohere Setup and Query Embedding\n\nInitializes the Cohere client with an API key, generates an embedding for a query, and performs a vector search in the Pinecone index to find similar documents","metadata":{}},{"cell_type":"code","source":"import cohere\nfrom load_dotenv import load_dotenv\nload_dotenv()\n\nCOHERE_API_KEY=getenv(\"COHERE_API_KEY\")\n# init client\nco = cohere.Client(COHERE_API_KEY)\n\nquery=\"what is 2 factor authentication?\"\n\nquestion_embedding=get_query_embdedding(query)\n\nquery_result = index.query(vector=question_embedding, top_k=5, include_metadata=True)\nsimilar_texts = []\n# Extract metadata from query result\ndocs = {x[\"metadata\"]['text']: i for i, x in enumerate(query_result[\"matches\"])}\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Document Re-ranking with Cohere\n\nUses Cohere's re-ranking model to refine the search results based on relevance to the query, then prepares a template for generating a summary.","metadata":{}},{"cell_type":"code","source":"# Rerank the documents\nrerank_docs = co.rerank(\n    model=\"rerank-english-v3.0\",\n    query=query, \n    documents=list(docs.keys()), \n    top_n=5, \n    return_documents=True\n)\n# print(\"rerank_docs...\",rerank_docs)\n\n# Extract reranked documents\nreranked_texts = [doc.document.text for doc in rerank_docs.results]\nprint(reranked_texts)\n\ncontext=\" \".join(reranked_texts)\n\nTemplate = f\"Based on the following context : {context} generate precise summary related to question : {query} Do not remove necessary information related to context. Consider `\\n` as newline character.\"  \n# Filling the template with the actual context and question.\nfilled_template = Template.format(context=context, question=query)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Uses Cohere's re-ranking model to refine the search results based on relevance to the query, then prepares a template for generating a summary.","metadata":{}},{"cell_type":"markdown","source":"# Generate Summary with Groq\n\nConfigures the Groq client, sends the filled template to the chat model for processing, and prints the generated summary based on the context and query provided.","metadata":{}},{"cell_type":"code","source":"from os import getenv\nfrom groq import Groq\nfrom load_dotenv import load_dotenv\nload_dotenv()\n\nGROQ_API_KEY=getenv(\"GROQ_API_KEY\")\n\nclient = Groq(\n    api_key=GROQ_API_KEY ,\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": filled_template,\n        }\n    ],\n    model=\"llama3-70b-8192\",\n)\n\nprint(chat_completion.choices[0].message.content)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nIn this notebook, we successfully demonstrated the advanced implementation of the Retrieval-Augmented Generation (RAG) technique, complemented by document re-ranking, to generate precise summaries from a given query. Through the integration of cutting-edge technologies and platforms such as Groq, Ollama, Pinecone, and Cohere, we showcased a sophisticated system capable of enhancing information retrieval and summarization tasks.\n\nKey takeaways include:\n- The ability to process and split PDF text into manageable chunks for further analysis.\n- The use of Ollama embeddings to convert text chunks into vector representations, facilitating efficient document retrieval.\n- The application of Pinecone's vector database for storing and querying document embeddings, enabling fast and scalable searches.\n- The implementation of Cohere's re-ranking model to refine search results, ensuring the selection of the most relevant documents.\n- The generation of comprehensive summaries using the Groq language model, based on the context provided by re-ranked documents.\n\nThis notebook not only serves as a practical guide to implementing a RAG system with re-ranking but also illustrates the power of combining multiple AI and NLP technologies to solve complex problems in information retrieval and summarization. We hope this demonstration inspires further exploration and development of advanced NLP applications.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}