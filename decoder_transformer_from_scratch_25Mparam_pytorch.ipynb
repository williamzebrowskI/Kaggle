{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8797926,"sourceType":"datasetVersion","datasetId":5290276}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/william2020/decoder-transformer-from-scratch-python-25m-param?scriptVersionId=185666977\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install torch colorama tdqm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-27T02:54:31.257042Z","iopub.execute_input":"2024-06-27T02:54:31.257843Z","iopub.status.idle":"2024-06-27T02:54:47.032399Z","shell.execute_reply.started":"2024-06-27T02:54:31.257799Z","shell.execute_reply":"2024-06-27T02:54:47.031392Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (0.4.6)\nCollecting tdqm\n  Downloading tdqm-0.0.1.tar.gz (1.4 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.3.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from tdqm) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nBuilding wheels for collected packages: tdqm\n  Building wheel for tdqm (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for tdqm: filename=tdqm-0.0.1-py3-none-any.whl size=1322 sha256=27b77961814fb53fb8e297e7dd5dcec1d0e873ae2bd6ba09f859fe274231cd44\n  Stored in directory: /root/.cache/pip/wheels/37/31/b8/7b711038035720ba0df14376af06e5e76b9bd61759c861ad92\nSuccessfully built tdqm\nInstalling collected packages: tdqm\nSuccessfully installed tdqm-0.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport math\nfrom tqdm import tqdm\nfrom colorama import Fore, Style, init","metadata":{"execution":{"iopub.status.busy":"2024-06-27T02:54:47.0344Z","iopub.execute_input":"2024-06-27T02:54:47.034679Z","iopub.status.idle":"2024-06-27T02:54:50.706615Z","shell.execute_reply.started":"2024-06-27T02:54:47.034653Z","shell.execute_reply":"2024-06-27T02:54:50.70558Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Initialize colorama\ninit(autoreset=True)\n\n# Constants\nNUM_EPOCHS = 10\nBATCH_SIZE = 32\nEMBEDDING_SIZE = 512\nNHEAD = 8\nFFN_HID_DIM = 512\nNUM_DECODER_LAYERS = 3\nMAX_SEQ_LENGTH = 128","metadata":{"execution":{"iopub.status.busy":"2024-06-27T02:54:50.711529Z","iopub.execute_input":"2024-06-27T02:54:50.71181Z","iopub.status.idle":"2024-06-27T02:54:50.71642Z","shell.execute_reply.started":"2024-06-27T02:54:50.711786Z","shell.execute_reply":"2024-06-27T02:54:50.715532Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def create_vocab(text):\n    \"\"\"\n    Create a vocabulary from the input text.\n    \n    Args:\n        text (str): The input text.\n    \n    Returns:\n        dict: A dictionary mapping words to unique indices.\n    \"\"\"\n    words = text.split()\n    unique_words = set(words)\n    vocab = {word: i+4 for i, word in enumerate(unique_words)}\n    vocab['<pad>'] = 0\n    vocab['<unk>'] = 1\n    vocab['<sos>'] = 2\n    vocab['<eos>'] = 3\n    return vocab","metadata":{"execution":{"iopub.status.busy":"2024-06-27T02:54:50.717418Z","iopub.execute_input":"2024-06-27T02:54:50.718016Z","iopub.status.idle":"2024-06-27T02:54:50.727858Z","shell.execute_reply.started":"2024-06-27T02:54:50.717991Z","shell.execute_reply":"2024-06-27T02:54:50.72705Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class TextDataset(Dataset):\n    \"\"\"\n    A custom dataset class for loading text data.\n    \"\"\"\n    def __init__(self, filepath, vocab=None):\n        \"\"\"\n        Initialize the dataset.\n        \n        Args:\n            filepath (str): Path to the text file.\n            vocab (dict, optional): Predefined vocabulary. If None, a new vocabulary is created.\n        \"\"\"\n        with open(filepath, 'r', encoding='utf-8') as file:\n            text = file.read().replace('\\n', ' ')\n\n        if vocab is None:\n            self.vocab = create_vocab(text)\n        else:\n            self.vocab = vocab\n        \n        self.data = [self.vocab.get(word, self.vocab['<unk>']) for word in text.split()]\n        self.data += [self.vocab['<eos>']] * MAX_SEQ_LENGTH\n        \n    def __len__(self):\n        \"\"\"\n        Return the length of the dataset.\n        \n        Returns:\n            int: The number of sequences in the dataset.\n        \"\"\"\n        return len(self.data) - MAX_SEQ_LENGTH + 1\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Get a single sequence from the dataset.\n        \n        Args:\n            idx (int): Index of the sequence.\n        \n        Returns:\n            tuple: A tuple containing the input sequence and the target sequence.\n        \"\"\"\n        sequence = self.data[idx:idx+MAX_SEQ_LENGTH]\n        input_sequence = torch.tensor(sequence[:-1], dtype=torch.long)\n        target_sequence = torch.tensor(sequence[1:], dtype=torch.long)\n        return input_sequence, target_sequence","metadata":{"execution":{"iopub.status.busy":"2024-06-27T02:54:50.728723Z","iopub.execute_input":"2024-06-27T02:54:50.729024Z","iopub.status.idle":"2024-06-27T02:54:50.741176Z","shell.execute_reply.started":"2024-06-27T02:54:50.729002Z","shell.execute_reply":"2024-06-27T02:54:50.740275Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class TransformerModel(nn.Module):\n    \"\"\"\n    A decoder-only Transformer model for text generation.\n    \"\"\"\n    def __init__(self, vocab_size):\n        \"\"\"\n        Initialize the Transformer model.\n        \n        Args:\n            vocab_size (int): The size of the vocabulary.\n        \"\"\"\n        super(TransformerModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n        self.pos_encoder = PositionalEncoding(EMBEDDING_SIZE, MAX_SEQ_LENGTH)\n        self.transformer_decoder_layer = nn.TransformerDecoderLayer(\n            d_model=EMBEDDING_SIZE, nhead=NHEAD, dim_feedforward=FFN_HID_DIM\n        )\n        self.transformer_decoder = nn.TransformerDecoder(\n            self.transformer_decoder_layer, num_layers=NUM_DECODER_LAYERS\n        )\n        self.fc_out = nn.Linear(EMBEDDING_SIZE, vocab_size)\n\n    def generate_square_subsequent_mask(self, sz):\n        \"\"\"\n        Generate a square mask for the sequence to prevent attending to future tokens.\n        \n        Args:\n            sz (int): The size of the mask.\n        \n        Returns:\n            torch.Tensor: The generated mask.\n        \"\"\"\n        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n        return mask\n\n    def forward(self, src):\n        \"\"\"\n        Forward pass of the Transformer model.\n        \n        Args:\n            src (torch.Tensor): The input sequence.\n        \n        Returns:\n            torch.Tensor: The output logits for the next token prediction.\n        \"\"\"\n        src_mask = self.generate_square_subsequent_mask(src.size(0)).to(src.device)\n        src = self.embedding(src) * math.sqrt(EMBEDDING_SIZE)\n        src = self.pos_encoder(src)\n        # Pass src as memory. In an autoregressive model, this is equivalent to the decoder attending to itself.\n        output = self.transformer_decoder(tgt=src, memory=src, tgt_mask=src_mask)\n        output = self.fc_out(output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-06-27T02:54:50.742111Z","iopub.execute_input":"2024-06-27T02:54:50.742425Z","iopub.status.idle":"2024-06-27T02:54:50.757202Z","shell.execute_reply.started":"2024-06-27T02:54:50.742402Z","shell.execute_reply":"2024-06-27T02:54:50.75629Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \"\"\"\n    Positional encoding module to add positional information to the embeddings.\n    \"\"\"\n    def __init__(self, d_model, max_len=5000):\n        \"\"\"\n        Initialize the positional encoding.\n        \n        Args:\n            d_model (int): The dimension of the model.\n            max_len (int): The maximum length of the sequences.\n        \"\"\"\n        super(PositionalEncoding, self).__init__()\n        self.encoding = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        self.encoding[:, 0::2] = torch.sin(position * div_term)\n        self.encoding[:, 1::2] = torch.cos(position * div_term)\n        self.encoding = self.encoding.unsqueeze(0)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass to add positional encoding to the input embeddings.\n        \n        Args:\n            x (torch.Tensor): The input embeddings.\n        \n        Returns:\n            torch.Tensor: The embeddings with added positional encoding.\n        \"\"\"\n        return x + self.encoding[:, :x.size(1)]","metadata":{"execution":{"iopub.status.busy":"2024-06-27T02:54:50.758356Z","iopub.execute_input":"2024-06-27T02:54:50.75869Z","iopub.status.idle":"2024-06-27T02:54:50.772576Z","shell.execute_reply.started":"2024-06-27T02:54:50.758658Z","shell.execute_reply":"2024-06-27T02:54:50.771686Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def count_parameters(model):\n    \"\"\"\n    Count the number of trainable parameters in the model.\n    \n    Args:\n        model (nn.Module): The model.\n    \n    Returns:\n        int: The number of trainable parameters.\n    \"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T02:54:50.773598Z","iopub.execute_input":"2024-06-27T02:54:50.773851Z","iopub.status.idle":"2024-06-27T02:54:50.785725Z","shell.execute_reply.started":"2024-06-27T02:54:50.773829Z","shell.execute_reply":"2024-06-27T02:54:50.784952Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def main():\n    \"\"\"\n    Main function to prepare the dataset, initialize the model, and train it.\n    \"\"\"\n    # Prepare dataset\n    dataset = TextDataset(\"/kaggle/input/p-and-pbook/book.txt\")\n    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n    # Initialize model\n    model = TransformerModel(len(dataset.vocab))\n\n    # Count parameters\n    total_params = count_parameters(model)\n    print(f\"The model has {total_params:,} trainable parameters\")\n    \n    model.train()\n\n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Training loop\n    for epoch in range(NUM_EPOCHS):\n        total_loss = 0\n        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f'Epoch {epoch+1}/{NUM_EPOCHS}', leave=True)\n        for i, (src, tgt) in progress_bar:\n            src = src.transpose(0, 1)\n            tgt_output = tgt.transpose(0, 1)\n            optimizer.zero_grad()\n            output = model(src)\n            loss = criterion(output.view(-1, len(dataset.vocab)), tgt_output.reshape(-1))\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            \n            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n\n    # Save the model\n    torch.save(model.state_dict(), 'transformer_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-27T02:54:50.787904Z","iopub.execute_input":"2024-06-27T02:54:50.78828Z","iopub.status.idle":"2024-06-27T02:54:50.797629Z","shell.execute_reply.started":"2024-06-27T02:54:50.788257Z","shell.execute_reply":"2024-06-27T02:54:50.796676Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T02:55:00.442844Z","iopub.execute_input":"2024-06-27T02:55:00.44368Z","iopub.status.idle":"2024-06-27T02:55:46.081522Z","shell.execute_reply.started":"2024-06-27T02:55:00.443646Z","shell.execute_reply":"2024-06-27T02:55:46.080119Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"The model has 25,596,278 trainable parameters\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   0%|          | 9/4076 [00:43<5:25:31,  4.80s/it, loss=7.1285]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[9], line 32\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m output \u001b[38;5;241m=\u001b[39m model(src)\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mvocab)), tgt_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}