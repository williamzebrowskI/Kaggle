{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/william2020/apple-s-mlx-transformers-pe-multi-head-attn?scriptVersionId=187548653\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Apple's MLX: Input/Positional Embeddings and Multi-head Self-Attention for the Transformer architecture\n\n#### Framework created by Apple.","metadata":{}},{"cell_type":"markdown","source":"In this tutorial, we will walk you through the core concepts and functionalities of MLX, starting with the basics of tokenization and positional encoding. Whether you’re a beginner looking to get started with machine learning on Apple hardware or an experienced practitioner seeking to optimize your workflows, this book provides practical examples and step-by-step instructions to help you harness the full potential of MLX.","metadata":{}},{"cell_type":"code","source":"!pip install -q mlx","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:33:44.617594Z","iopub.execute_input":"2024-07-09T16:33:44.61806Z","iopub.status.idle":"2024-07-09T16:34:02.824805Z","shell.execute_reply.started":"2024-07-09T16:33:44.617996Z","shell.execute_reply":"2024-07-09T16:34:02.823286Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport mlx.core as mx\nimport mlx.nn as nn\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:02.827552Z","iopub.execute_input":"2024-07-09T16:34:02.82804Z","iopub.status.idle":"2024-07-09T16:34:04.223635Z","shell.execute_reply.started":"2024-07-09T16:34:02.827999Z","shell.execute_reply":"2024-07-09T16:34:04.222641Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Step 1: Tokenize the sentence","metadata":{}},{"cell_type":"code","source":"sentence = \"What are the advantages and disadvantages of using a unified memory architecture?\"\ntokens = word_tokenize(sentence.lower())\nprint(\"Tokens:\", tokens)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:04.229238Z","iopub.execute_input":"2024-07-09T16:34:04.229629Z","iopub.status.idle":"2024-07-09T16:34:04.252114Z","shell.execute_reply.started":"2024-07-09T16:34:04.229598Z","shell.execute_reply":"2024-07-09T16:34:04.250955Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Tokens: ['what', 'are', 'the', 'advantages', 'and', 'disadvantages', 'of', 'using', 'a', 'unified', 'memory', 'architecture', '?']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Step 2: Create a simple vocabulary and convert tokens to indices","metadata":{}},{"cell_type":"code","source":"vocab = defaultdict(lambda: len(vocab))\nindices = [vocab[token] for token in tokens]\nprint(\"Indices:\", indices)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:04.253557Z","iopub.execute_input":"2024-07-09T16:34:04.254133Z","iopub.status.idle":"2024-07-09T16:34:04.260338Z","shell.execute_reply.started":"2024-07-09T16:34:04.254101Z","shell.execute_reply":"2024-07-09T16:34:04.259176Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Step 3: Initialize the embedding layer","metadata":{}},{"cell_type":"code","source":"embedding_dim = 64\nnum_embeddings = len(vocab)\nembedding_layer = nn.Embedding(num_embeddings, embedding_dim)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:04.261724Z","iopub.execute_input":"2024-07-09T16:34:04.262149Z","iopub.status.idle":"2024-07-09T16:34:04.271362Z","shell.execute_reply.started":"2024-07-09T16:34:04.26211Z","shell.execute_reply":"2024-07-09T16:34:04.270178Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Convert indices to MLX array and embed them","metadata":{}},{"cell_type":"code","source":"input_data = mx.array(indices)\nembedded_tokens = embedding_layer(input_data)\nprint(\"Embedded Tokens:\", embedded_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:04.272854Z","iopub.execute_input":"2024-07-09T16:34:04.273228Z","iopub.status.idle":"2024-07-09T16:34:04.283453Z","shell.execute_reply.started":"2024-07-09T16:34:04.273199Z","shell.execute_reply":"2024-07-09T16:34:04.282414Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Embedded Tokens: array([[-0.241889, 0.0479568, 0.0795218, ..., 0.0124098, 0.0172652, 0.0071785],\n       [0.141629, 0.167849, 0.0870575, ..., -0.0376079, -0.191881, 0.148116],\n       [0.0208214, 0.00694565, -0.161432, ..., 0.0463976, -0.00566133, -0.160316],\n       ...,\n       [0.020175, 0.0723507, 0.174394, ..., -0.0944999, -0.113024, -0.164732],\n       [-0.220407, -0.0641856, -0.00881927, ..., 0.0507493, 0.0249283, 0.229439],\n       [0.013175, 0.064244, 0.0242759, ..., 0.0134159, 0.147243, -0.0463119]], dtype=float32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RoPE (Rotary Positional Encoding)\n\nRoPE applies a rotational transformation to the token embeddings based on their positions in the sequence. This transformation uses sinusoidal functions to create a set of rotation matrices that are applied to the embeddings. The result is a set of positionally encoded embeddings that carry rich relative positional information.\n\nIn transformer models, understanding the relative positions of tokens within a sequence is crucial for tasks that require contextual understanding, such as language modeling and translation.","metadata":{}},{"cell_type":"markdown","source":"# Calculate sequence length","metadata":{}},{"cell_type":"code","source":"seq_len = embedded_tokens.shape[0]\nprint(\"Sequence Length:\", seq_len)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:04.28493Z","iopub.execute_input":"2024-07-09T16:34:04.285396Z","iopub.status.idle":"2024-07-09T16:34:04.296982Z","shell.execute_reply.started":"2024-07-09T16:34:04.285366Z","shell.execute_reply":"2024-07-09T16:34:04.29582Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Sequence Length: 13\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Generate frequencies for the sinusoidal embeddings","metadata":{}},{"cell_type":"code","source":"inv_freq = 1.0 / (10000 ** (np.arange(0, embedding_dim, 2).astype(np.float32) / embedding_dim))\nprint(\"Inverse Frequencies:\", inv_freq)\n\nfreqs = mx.array(np.outer(np.arange(seq_len), inv_freq).astype(np.float32))\nprint(\"Frequencies:\", freqs)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:04.298391Z","iopub.execute_input":"2024-07-09T16:34:04.298774Z","iopub.status.idle":"2024-07-09T16:34:04.311125Z","shell.execute_reply.started":"2024-07-09T16:34:04.298735Z","shell.execute_reply":"2024-07-09T16:34:04.310049Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Inverse Frequencies: [1.0000000e+00 7.4989420e-01 5.6234133e-01 4.2169651e-01 3.1622776e-01\n 2.3713736e-01 1.7782794e-01 1.3335215e-01 1.0000000e-01 7.4989416e-02\n 5.6234129e-02 4.2169649e-02 3.1622779e-02 2.3713736e-02 1.7782794e-02\n 1.3335215e-02 9.9999998e-03 7.4989423e-03 5.6234132e-03 4.2169648e-03\n 3.1622779e-03 2.3713738e-03 1.7782794e-03 1.3335214e-03 1.0000000e-03\n 7.4989418e-04 5.6234130e-04 4.2169649e-04 3.1622779e-04 2.3713738e-04\n 1.7782794e-04 1.3335215e-04]\nFrequencies: array([[0, 0, 0, ..., 0, 0, 0],\n       [1, 0.749894, 0.562341, ..., 0.000237137, 0.000177828, 0.000133352],\n       [2, 1.49979, 1.12468, ..., 0.000474275, 0.000355656, 0.000266704],\n       ...,\n       [10, 7.49894, 5.62341, ..., 0.00237137, 0.00177828, 0.00133352],\n       [11, 8.24884, 6.18575, ..., 0.00260851, 0.00195611, 0.00146687],\n       [12, 8.99873, 6.7481, ..., 0.00284565, 0.00213394, 0.00160023]], dtype=float32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Calculate cosine and sine of the frequencies","metadata":{}},{"cell_type":"code","source":"cos_pos = mx.cos(freqs)\nsin_pos = mx.sin(freqs)\nprint(\"Cosine Positional Encoding:\", cos_pos)\nprint(\"Sine Positional Encoding:\", sin_pos)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:04.31618Z","iopub.execute_input":"2024-07-09T16:34:04.316631Z","iopub.status.idle":"2024-07-09T16:34:04.32481Z","shell.execute_reply.started":"2024-07-09T16:34:04.316596Z","shell.execute_reply":"2024-07-09T16:34:04.323443Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Cosine Positional Encoding: array([[1, 1, 1, ..., 1, 1, 1],\n       [0.540302, 0.731761, 0.846009, ..., 1, 1, 1],\n       [-0.416147, 0.0709483, 0.431463, ..., 1, 1, 1],\n       ...,\n       [-0.839072, 0.347628, 0.790132, ..., 0.999997, 0.999998, 0.999999],\n       [0.0044257, -0.384674, 0.995257, ..., 0.999997, 0.999998, 0.999999],\n       [0.843854, -0.910606, 0.893862, ..., 0.999996, 0.999998, 0.999999]], dtype=float32)\nSine Positional Encoding: array([[0, 0, 0, ..., 0, 0, 0],\n       [0.841471, 0.681561, 0.533168, ..., 0.000237137, 0.000177828, 0.000133352],\n       [0.909297, 0.99748, 0.902131, ..., 0.000474275, 0.000355656, 0.000266704],\n       ...,\n       [-0.544021, 0.937633, -0.612937, ..., 0.00237137, 0.00177828, 0.00133352],\n       [-0.99999, 0.923052, -0.0972765, ..., 0.00260851, 0.00195611, 0.00146687],\n       [-0.536573, 0.413275, 0.448343, ..., 0.00284564, 0.00213393, 0.00160023]], dtype=float32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Split embedded tokens into even and odd parts\n\n Splits the embedded tokens into even and odd parts.","metadata":{}},{"cell_type":"code","source":"x1 = embedded_tokens[:, ::2]\nx2 = embedded_tokens[:, 1::2]\nprint(\"x1 (even):\", x1.shape)\nprint(\"x2 (odd):\", x2.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:04.326622Z","iopub.execute_input":"2024-07-09T16:34:04.327067Z","iopub.status.idle":"2024-07-09T16:34:04.338161Z","shell.execute_reply.started":"2024-07-09T16:34:04.327028Z","shell.execute_reply":"2024-07-09T16:34:04.33694Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"x1 (even): (13, 32)\nx2 (odd): (13, 32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Apply rotational transformation\n\nApplies the rotational transformation to the split parts.","metadata":{}},{"cell_type":"code","source":"x1_new = x1 * cos_pos - x2 * sin_pos\nx2_new = x1 * sin_pos + x2 * cos_pos\nprint(\"Transformed x1:\", x1_new.shape)\nprint(\"Transformed x2:\", x2_new.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:04.339671Z","iopub.execute_input":"2024-07-09T16:34:04.340093Z","iopub.status.idle":"2024-07-09T16:34:04.353733Z","shell.execute_reply.started":"2024-07-09T16:34:04.340016Z","shell.execute_reply":"2024-07-09T16:34:04.352334Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Transformed x1: (13, 32)\nTransformed x2: (13, 32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Concatenate the new x1 and x2 back together\n\nConcatenates the transformed parts back together to get the final positional encoded embeddings.","metadata":{}},{"cell_type":"code","source":"positional_encoded_embeddings = mx.concatenate([x1_new, x2_new], axis=-1)\nprint(\"Positional Encoded Embeddings:\", positional_encoded_embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:04.355454Z","iopub.execute_input":"2024-07-09T16:34:04.3562Z","iopub.status.idle":"2024-07-09T16:34:04.366511Z","shell.execute_reply.started":"2024-07-09T16:34:04.356156Z","shell.execute_reply":"2024-07-09T16:34:04.365315Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Positional Encoded Embeddings: array([[-0.241889, 0.0795218, 0.0822457, ..., 0.189025, 0.0124098, 0.0071785],\n       [-0.0647175, 0.0345135, 0.040535, ..., 0.0525666, -0.0375987, 0.14809],\n       [-0.0149804, 0.0733039, 0.0653143, ..., -0.102836, 0.0464649, -0.160317],\n       ...,\n       [0.022432, 0.0585029, 0.149466, ..., 0.117419, -0.0942642, -0.164882],\n       [-0.0651605, -0.116964, 0.0153244, ..., -0.269856, 0.0503992, 0.229476],\n       [0.0455894, -0.0800577, 0.0880328, ..., 0.00635958, 0.0129789, -0.0460762]], dtype=float32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Multi-Head Self-Attention\n\nTo enhance the model’s ability to capture different types of relationships, the transformer employs multi-head self-attention. This technique splits the Query, Key, and Value vectors into multiple smaller sub-vectors, each corresponding to a different attention head. The attention mechanism is applied independently to each head, and the results are concatenated and linearly transformed to produce the final output.\n","metadata":{}},{"cell_type":"code","source":"# Define the dimensions\nnum_heads = 8\nhead_dim = embedding_dim // num_heads\n\n# Sample input embeddings after positional encoding (from previous section)\n# For demonstration, we assume `positional_encoded_embeddings` is already defined\ninput_embeddings = positional_encoded_embeddings\n\n# Add a batch dimension if missing\nif len(input_embeddings.shape) == 2:\n    input_embeddings = input_embeddings[np.newaxis, :, :]\n\nprint(\"Input Embeddings Shape:\", input_embeddings.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:04.368812Z","iopub.execute_input":"2024-07-09T16:34:04.369286Z","iopub.status.idle":"2024-07-09T16:34:04.380212Z","shell.execute_reply.started":"2024-07-09T16:34:04.369225Z","shell.execute_reply":"2024-07-09T16:34:04.378973Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Input Embeddings Shape: (1, 13, 64)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Linear Transformations\n\nThis cell applies linear projections to the input embeddings to obtain the Query, Key, and Value matrices.","metadata":{}},{"cell_type":"code","source":"# Step 1: Linear Transformations\nquery_proj = nn.Linear(embedding_dim, embedding_dim, bias=False)\nkey_proj = nn.Linear(embedding_dim, embedding_dim, bias=False)\nvalue_proj = nn.Linear(embedding_dim, embedding_dim, bias=False)\n\nqueries = query_proj(input_embeddings)\nkeys = key_proj(input_embeddings)\nvalues = value_proj(input_embeddings)\n\nprint(\"Queries Shape:\", queries.shape)\nprint(\"Keys Shape:\", keys.shape)\nprint(\"Values Shape:\", values.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:04.382232Z","iopub.execute_input":"2024-07-09T16:34:04.382803Z","iopub.status.idle":"2024-07-09T16:34:04.3957Z","shell.execute_reply.started":"2024-07-09T16:34:04.382762Z","shell.execute_reply":"2024-07-09T16:34:04.394589Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Queries Shape: (1, 13, 64)\nKeys Shape: (1, 13, 64)\nValues Shape: (1, 13, 64)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Split for Multi-Head Attention\n\nThis cell reshapes the Query, Key, and Value matrices to prepare them for multi-head attention. It then transposes these matrices to separate the attention heads and prints their shapes.","metadata":{}},{"cell_type":"code","source":"batch_size, seq_length, _ = queries.shape\n\nqueries = queries.reshape(batch_size, seq_length, num_heads, head_dim).transpose(0, 2, 1, 3)\nkeys = keys.reshape(batch_size, seq_length, num_heads, head_dim).transpose(0, 2, 1, 3)\nvalues = values.reshape(batch_size, seq_length, num_heads, head_dim).transpose(0, 2, 1, 3)\n\nprint(\"Split Queries Shape:\", queries.shape)\nprint(\"Split Keys Shape:\", keys.shape)\nprint(\"Split Values Shape:\", values.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:04.397468Z","iopub.execute_input":"2024-07-09T16:34:04.398393Z","iopub.status.idle":"2024-07-09T16:34:04.406539Z","shell.execute_reply.started":"2024-07-09T16:34:04.39836Z","shell.execute_reply":"2024-07-09T16:34:04.405441Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Split Queries Shape: (1, 8, 13, 8)\nSplit Keys Shape: (1, 8, 13, 8)\nSplit Values Shape: (1, 8, 13, 8)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Scaled Dot-Product Attention\n\nThis cell calculates the attention scores by performing scaled dot-product attention. It then applies the softmax function to obtain normalized attention weights and computes the final attention output.","metadata":{}},{"cell_type":"code","source":"# Step 3: Scaled Dot-Product Attention\ndk = head_dim\nscores = mx.matmul(queries, keys.transpose(0, 1, 3, 2)) / mx.sqrt(mx.array([dk], dtype=queries.dtype))\nattention_weights = nn.softmax(scores, axis=-1)\nattention_output = mx.matmul(attention_weights, values)\n\nprint(\"Attention Weights Shape:\", attention_weights.shape)\nprint(\"Attention Output Shape:\", attention_output.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:04.408157Z","iopub.execute_input":"2024-07-09T16:34:04.408513Z","iopub.status.idle":"2024-07-09T16:34:04.41903Z","shell.execute_reply.started":"2024-07-09T16:34:04.408483Z","shell.execute_reply":"2024-07-09T16:34:04.417946Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Attention Weights Shape: (1, 8, 13, 13)\nAttention Output Shape: (1, 8, 13, 8)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Combine Heads\n\nThis cell combines the output from all attention heads back into a single tensor. It reshapes the combined output to match the original embedding dimensions and prints the shape.","metadata":{}},{"cell_type":"code","source":"# Step 4: Combine Heads\ncombined_output = attention_output.transpose(0, 2, 1, 3).reshape(batch_size, seq_length, embedding_dim)\nprint(\"Combined Output Shape:\", combined_output.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:04.420305Z","iopub.execute_input":"2024-07-09T16:34:04.420645Z","iopub.status.idle":"2024-07-09T16:34:04.431288Z","shell.execute_reply.started":"2024-07-09T16:34:04.420616Z","shell.execute_reply":"2024-07-09T16:34:04.430225Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Combined Output Shape: (1, 13, 64)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Final Linear Transformation\n\nThis cell applies a final linear transformation to the combined output from the attention heads.","metadata":{}},{"cell_type":"code","source":"# Step 5: Final Linear Transformation\noutput_proj = nn.Linear(embedding_dim, embedding_dim, bias=False)\nfinal_output = output_proj(combined_output)\nprint(\"Final Output after Self-Attention Shape:\", final_output.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:34:04.432881Z","iopub.execute_input":"2024-07-09T16:34:04.433239Z","iopub.status.idle":"2024-07-09T16:34:04.443713Z","shell.execute_reply.started":"2024-07-09T16:34:04.433198Z","shell.execute_reply":"2024-07-09T16:34:04.44251Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Final Output after Self-Attention Shape: (1, 13, 64)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Conclusion\n\nIn this notebook, we explored the foundational components of the transformer model architecture using the MLX framework, specifically tailored for Apple Silicon. Here’s a summary of what we covered:\n\n1.\tTokenization and Embedding:\n\n\t- We began by tokenizing an example sentence into individual tokens using NLTK.\n\t- Each token was then converted into a unique index using a simple vocabulary.\n\t- We utilized an embedding layer to convert these token indices into dense vectors, preparing them for \t   further processing.\n2.\tRotary Position Embedding (RoPE):\n\n\t- We applied the RoPE technique to enhance the input embeddings with positional information.\n\t- RoPE uses a rotational transformation based on sinusoidal functions to incorporate relative positional data directly into the embeddings.\n\t\n3. Self-Attention Mechanism:\n\t- Following positional encoding, we implemented the self-attention mechanism, a core component of transformer models.\n\t\t- This included:\n\t\t- Linear transformations to obtain Query, Key, and Value matrices from the input embeddings.\n\t\t- Splitting these matrices into multiple heads for multi-head attention.\n\t\t- Calculating attention scores through scaled dot-product attention and applying the softmax function to obtain normalized attention weights.\n\t\t- Combining the output from all attention heads and applying a final linear transformation to produce the final self-attention output.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}