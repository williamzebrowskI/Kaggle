{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/william2020/multi-layer-perception-in-mlx-explained?scriptVersionId=187289609\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Multi-layer Perception in MLX Explained\n\nIn this notebook, we will go through the `MLP` class definition line by line to understand how it constructs a Multi-Layer Perceptron (MLP) using the MLX framework.\n\n### Multi-Layer Perceptrons (MLPs)\nA Multi-Layer Perceptron (MLP) is a type of feedforward artificial neural network that consists of multiple layers of neurons, including an input layer, one or more hidden layers, and an output layer. Each neuron in a layer is connected to every neuron in the subsequent layer, and non-linear activation functions are applied to introduce complexity into the model. MLPs are widely used for various tasks such as classification, regression, and pattern recognition due to their ability to learn complex relationships in data.\n\n### MLX Framework\nMLX is a high-performance array computing library designed specifically for machine learning on Apple silicon. It provides a NumPy-like API and supports automatic differentiation, lazy computation, and seamless execution on both CPU and GPU. MLX leverages the unified memory architecture of Apple devices, allowing efficient computation without the need for explicit data transfers between CPU and GPU, making it an excellent choice for developing and training machine learning models.\n\nBy combining the strengths of MLPs with the efficiency of the MLX framework, we can build powerful neural network models optimized for Apple's hardware.","metadata":{}},{"cell_type":"markdown","source":"### First, let's start off with the full MPP class...","metadata":{}},{"cell_type":"code","source":"!pip install mlx","metadata":{"execution":{"iopub.status.busy":"2024-07-08T00:39:23.950288Z","iopub.execute_input":"2024-07-08T00:39:23.950665Z","iopub.status.idle":"2024-07-08T00:39:36.32071Z","shell.execute_reply.started":"2024-07-08T00:39:23.950635Z","shell.execute_reply":"2024-07-08T00:39:36.319345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import mlx.nn as nn\nimport mlx","metadata":{"execution":{"iopub.status.busy":"2024-07-08T00:39:36.323092Z","iopub.execute_input":"2024-07-08T00:39:36.323483Z","iopub.status.idle":"2024-07-08T00:39:36.329059Z","shell.execute_reply.started":"2024-07-08T00:39:36.323448Z","shell.execute_reply":"2024-07-08T00:39:36.32767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(\n        self, num_layers: int, input_dim: int, hidden_dim: int, output_dim: int\n    ):\n        super().__init__()\n        layer_sizes = [input_dim] + [hidden_dim] * num_layers + [output_dim]\n        self.layers = [\n            nn.Linear(idim, odim)\n            for idim, odim in zip(layer_sizes[:-1], layer_sizes[1:])\n        ]\n\n    def __call__(self, x):\n        for l in self.layers[:-1]:\n            x = mx.maximum(l(x), 0.0)\n        return self.layers[-1](x)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T00:39:36.330662Z","iopub.execute_input":"2024-07-08T00:39:36.331113Z","iopub.status.idle":"2024-07-08T00:39:36.345207Z","shell.execute_reply.started":"2024-07-08T00:39:36.331073Z","shell.execute_reply":"2024-07-08T00:39:36.343986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's break it down this Multi Layer Perception down line by line","metadata":{}},{"cell_type":"markdown","source":"### 1. Class Definition\n\n```bash\n    class MLP(nn.Module):\n```\n\nDefines a new class MLP that inherits from nn.Module. This is the base class for all neural network modules in MLX.","metadata":{}},{"cell_type":"markdown","source":"### 2. Initialization Method\n\n```bash\n    def __init__(self, num_layers: int, input_dim: int, hidden_dim: int, output_dim: int):\n```\n\n- The __init__ method is the constructor of the class.\n\t- Parameters:\n        - num_layers: Number of hidden layers in the MLP.\n        - input_dim: Dimension of the input features.\n        - hidden_dim: Dimension of the hidden layers.\n        - output_dim: Dimension of the output layer.","metadata":{}},{"cell_type":"markdown","source":"### 3. Superclass Initialization\n\n```bash\n    super().__init__()\n```\n\nCalls the constructor of the superclass nn.Module to initialize the base class.","metadata":{}},{"cell_type":"markdown","source":"### 4. Layer Sizes Definition\n\n```bash\n    layer_sizes = [input_dim] + [hidden_dim] * num_layers + [output_dim]\n```\n\n- Creates a list layer_sizes that defines the sizes of each layer in the network.\n- Example: If input_dim=784, hidden_dim=128, num_layers=2, and output_dim=10, then layer_sizes will be [784, 128, 128, 10].","metadata":{}},{"cell_type":"markdown","source":"### 5. Creating the Layers\n\n```bash\n        self.layers = [\n            nn.Linear(idim, odim)\n            for idim, odim in zip(layer_sizes[:-1], layer_sizes[1:])\n        ]\n```\n\n- Uses a list comprehension to create the linear layers of the MLP.\n- zip(layer_sizes[:-1], layer_sizes[1:]) pairs each input dimension with the corresponding output dimension for each layer.\n- nn.Linear(idim, odim) creates a linear layer with input dimension idim and output dimension odim.\n- These layers are stored in self.layers.","metadata":{}},{"cell_type":"markdown","source":"### 6. Forward Method\n\n```bash\n    def __call__(self, x):\n```\n\nDefines the forward pass of the network. This method is called when the instance is used as a function, e.g., model(x).","metadata":{}},{"cell_type":"markdown","source":"### 7. Applying the Layers\n\n```bash\n        for l in self.layers[:-1]:\n            x = mx.maximum(l(x), 0.0)\n```\n\n- Iterates over all layers except the last one.\n- Applies each layer to the input x and uses the ReLU activation function (mx.maximum(l(x), 0.0)) to introduce non-linearity.","metadata":{}},{"cell_type":"markdown","source":"### 8. Final Layer\n\n```bash\n        return self.layers[-1](x)\n```\n\n- Applies the last layer to the input x and returns the output.\n- The last layer does not use an activation function, allowing the network to output raw scores for tasks like classification.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\nThe MLP class defines a simple multi-layer perceptron with a customizable number of hidden layers. It uses linear layers and ReLU activation functions, except for the final layer which outputs raw scores. This structure is typical for feedforward neural networks used in various machine learning tasks.","metadata":{}}]}